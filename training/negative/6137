<!DOCTYPE html>
<html lang="en" dir="ltr" class="client-nojs">
<head>
<meta charset="UTF-8" />
<title>Hierarchical clustering - Wikipedia, the free encyclopedia</title>
<meta name="generator" content="MediaWiki 1.26wmf13" />
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Hierarchical_clustering" />
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Hierarchical_clustering&amp;action=edit" />
<link rel="edit" title="Edit this page" href="/w/index.php?title=Hierarchical_clustering&amp;action=edit" />
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png" />
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico" />
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)" />
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd" />
<link rel="alternate" hreflang="x-default" href="/wiki/Hierarchical_clustering" />
<link rel="copyright" href="//creativecommons.org/licenses/by-sa/3.0/" />
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom" />
<link rel="canonical" href="https://en.wikipedia.org/wiki/Hierarchical_clustering" />
<link rel="stylesheet" href="//en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=ext.math.styles%7Cext.uls.nojs%7Cext.visualEditor.viewPageTarget.noscript%7Cext.wikihiero%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cmediawiki.ui.button%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector&amp;*" />
<meta name="ResourceLoaderDynamicStyles" content="" />
<link rel="stylesheet" href="//en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=site&amp;only=styles&amp;skin=vector&amp;*" />
<style>a:lang(ar),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: global:resourceloader:filter:minify-css:7:de1ab5287c9076b96eedd3f97a84a7b6 */</style>
<script src="//en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector&amp;*"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Hierarchical_clustering","wgTitle":"Hierarchical clustering","wgCurRevisionId":670589093,"wgRevisionId":670589093,"wgArticleId":477573,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from April 2009","Network analysis","Data clustering algorithms"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Hierarchical_clustering","wgRelevantArticleId":477573,"wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wikilove-recipient":"","wikilove-anon":0,"wgPoweredByHHVM":true,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"hidesig":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgGatherShouldShowTutorial":true,"wgGatherPageImageThumbnail":"//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/100px-Linear-svm-scatterplot.svg.png","wgULSAcceptLanguageList":["zh-cn","zh","en","zh-tw"],"wgULSCurrentAutonym":"English","wgFlaggedRevsParams":{"tags":{"status":{"levels":1,"quality":2,"pristine":3}}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgNoticeProject":"wikipedia","wgWikibaseItemId":"Q1277447"});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function($,jQuery){mw.user.options.set({"variant":"en"});});
/* cache key: global:resourceloader:filter:minify-js:7:b2706269305541eba923c165462b22c4 */
}</script>
<script>if(window.mw){
mw.loader.implement("user.tokens",function($,jQuery){mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\"});});
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","ext.centralauth.centralautologin","mmv.head","ext.imageMetrics.head","ext.visualEditor.viewPageTarget.init","ext.uls.init","ext.uls.interface","ext.centralNotice.bannerController","skins.vector.js"]);
}</script>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 7]><style type="text/css">body{behavior:url("/w/static/1.26wmf13/skins/Vector/csshover.min.htc")}</style><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Hierarchical_clustering skin-vector action-view">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><!-- CentralNotice --></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">Hierarchical clustering</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%">
<tr>
<th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br />
<a href="/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Linear-svm-scatterplot.svg" class="image" title="Scatterplot featuring a linear support vector machine's decision boundary (dashed line)"><img alt="Scatterplot featuring a linear support vector machine's decision boundary (dashed line)" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/220px-Linear-svm-scatterplot.svg.png" width="220" height="165" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/330px-Linear-svm-scatterplot.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/46/Linear-svm-scatterplot.svg/440px-Linear-svm-scatterplot.svg.png 2x" data-file-width="720" data-file-height="540" /></a></td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;">Problems</th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br />
<span style="font-weight:normal;"><small style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;• <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
</th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/BIRCH_(data_clustering)" title="BIRCH (data clustering)" class="mw-redirect">BIRCH</a></li>
<li><strong class="selflink">Hierarchical</strong></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm" class="mw-redirect">Expectation-maximization (EM)</a></li>
<li><br />
<a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean-shift" title="Mean-shift" class="mw-redirect">Mean-shift</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis" class="mw-redirect">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification" class="mw-redirect"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a></th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th style="padding:0.1em;border-top:1px solid #aaa;">Theory</th>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="hlist">
<ul>
<li><a href="/wiki/Bias-variance_dilemma" title="Bias-variance dilemma" class="mw-redirect">Bias-variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><span class="metadata"><a href="/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal icon" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a></span> <a href="/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li>
<li><span class="metadata"><a href="/wiki/File:Internet_map_1024.jpg" class="image"><img alt="Portal icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/16px-Internet_map_1024.jpg" width="16" height="16" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/24px-Internet_map_1024.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/32px-Internet_map_1024.jpg 2x" data-file-width="1280" data-file-height="1280" /></a></span> <a href="/wiki/Portal:Computer_science" title="Portal:Computer science">Computer science portal</a></li>
<li><span class="metadata"><a href="/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image"><img alt="Portal icon" src="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/16px-Fisher_iris_versicolor_sepalwidth.svg.png" width="16" height="11" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/24px-Fisher_iris_versicolor_sepalwidth.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/32px-Fisher_iris_versicolor_sepalwidth.svg.png 2x" data-file-width="822" data-file-height="567" /></a></span> <a href="/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><span title="View this template">v</span></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><span title="Discuss this template">t</span></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><span title="Edit this template">e</span></a></li>
</ul>
</div>
</td>
</tr>
</table>
<p>In <a href="/wiki/Data_mining" title="Data mining">data mining</a> and <a href="/wiki/Statistics" title="Statistics">statistics</a>, <b>hierarchical clustering</b> (also called <b>hierarchical cluster analysis</b> or <b>HCA</b>) is a method of <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> which seeks to build a <a href="/wiki/Hierarchy" title="Hierarchy">hierarchy</a> of clusters. Strategies for hierarchical clustering generally fall into two types: <sup id="cite_ref-1" class="reference"><a href="#cite_note-1"><span>[</span>1<span>]</span></a></sup></p>
<ul>
<li><b>Agglomerative</b>: This is a "bottom up" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.</li>
<li><b>Divisive</b>: This is a "top down" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.</li>
</ul>
<p>In general, the merges and splits are determined in a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a> manner. The results of hierarchical clustering are usually presented in a <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a>.</p>
<p>In the general case, the complexity of agglomerative clustering is <img class="mwe-math-fallback-image-inline tex" alt="O(n^3)" src="//upload.wikimedia.org/math/6/8/0/6809c59370e21b3e6e8fd117442fd377.png" />, which makes them too slow for large data sets. Divisive clustering with an exhaustive search is <img class="mwe-math-fallback-image-inline tex" alt="O(2^n)" src="//upload.wikimedia.org/math/6/e/2/6e2655c28b2a4d8f856641cc26bf6aa1.png" />, which is even worse. However, for some special cases, optimal efficient agglomerative methods (of complexity <img class="mwe-math-fallback-image-inline tex" alt="O(n^2)" src="//upload.wikimedia.org/math/1/8/9/189317b4b935a745fcfaf95940d2b4f0.png" />) are known: SLINK<sup id="cite_ref-SLINK_2-0" class="reference"><a href="#cite_note-SLINK-2"><span>[</span>2<span>]</span></a></sup> for single-linkage and CLINK<sup id="cite_ref-3" class="reference"><a href="#cite_note-3"><span>[</span>3<span>]</span></a></sup> for complete-linkage clustering.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Cluster_dissimilarity"><span class="tocnumber">1</span> <span class="toctext">Cluster dissimilarity</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Metric"><span class="tocnumber">1.1</span> <span class="toctext">Metric</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Linkage_criteria"><span class="tocnumber">1.2</span> <span class="toctext">Linkage criteria</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Discussion"><span class="tocnumber">2</span> <span class="toctext">Discussion</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Example_for_Agglomerative_Clustering"><span class="tocnumber">3</span> <span class="toctext">Example for Agglomerative Clustering</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Software"><span class="tocnumber">4</span> <span class="toctext">Software</span></a>
<ul>
<li class="toclevel-2 tocsection-7"><a href="#Open_Source_Frameworks"><span class="tocnumber">4.1</span> <span class="toctext">Open Source Frameworks</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Standalone_implementations"><span class="tocnumber">4.2</span> <span class="toctext">Standalone implementations</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Commercial"><span class="tocnumber">4.3</span> <span class="toctext">Commercial</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Notes"><span class="tocnumber">6</span> <span class="toctext">Notes</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#References_and_further_reading"><span class="tocnumber">7</span> <span class="toctext">References and further reading</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Cluster_dissimilarity">Cluster dissimilarity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=1" title="Edit section: Cluster dissimilarity">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), a measure of dissimilarity between sets of observations is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> (a measure of <a href="/wiki/Distance" title="Distance">distance</a> between pairs of observations), and a linkage criterion which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets.</p>
<h3><span class="mw-headline" id="Metric">Metric</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=2" title="Edit section: Metric">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="hatnote">Further information: <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric (mathematics)</a></div>
<p>The choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a 2-dimensional space, the distance between the point (1,0) and the origin (0,0) is always 1 according to the usual norms, but the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, <img class="mwe-math-fallback-image-inline tex" alt="\scriptstyle\sqrt{2}" src="//upload.wikimedia.org/math/b/f/c/bfc552de0f2e3353d96542d0e6382405.png" /> under Euclidean distance, or 1 under maximum distance.</p>
<p>Some commonly used metrics for hierarchical clustering are:<sup id="cite_ref-4" class="reference"><a href="#cite_note-4"><span>[</span>4<span>]</span></a></sup></p>
<table class="wikitable">
<tr>
<th>Names</th>
<th>Formula</th>
</tr>
<tr>
<td><a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \|a-b \|_2 = \sqrt{\sum_i (a_i-b_i)^2} " src="//upload.wikimedia.org/math/8/3/6/8369e1e3c67236a2dc8e3c27d9457d3e.png" /></td>
</tr>
<tr>
<td>Squared Euclidean distance</td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \|a-b \|_2^2 = \sum_i (a_i-b_i)^2 " src="//upload.wikimedia.org/math/a/3/2/a32988f2157530ab28abab9cd4c07fba.png" /></td>
</tr>
<tr>
<td><a href="/wiki/Manhattan_distance" title="Manhattan distance" class="mw-redirect">Manhattan distance</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \|a-b \|_1 = \sum_i |a_i-b_i| " src="//upload.wikimedia.org/math/b/8/0/b80e92f3b955a9bd22e3da58f53949ab.png" /></td>
</tr>
<tr>
<td><a href="/wiki/Uniform_norm" title="Uniform norm">maximum distance</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \|a-b \|_\infty = \max_i |a_i-b_i| " src="//upload.wikimedia.org/math/f/b/c/fbc4b1ccbdf1d1c120777898921a49eb.png" /></td>
</tr>
<tr>
<td><a href="/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \sqrt{(a-b)^{\top}S^{-1}(a-b)} " src="//upload.wikimedia.org/math/d/2/6/d264afd8ed35e18537011fec8370d19b.png" /> where <i>S</i> is the <a href="/wiki/Covariance_matrix" title="Covariance matrix">Covariance matrix</a></td>
</tr>
</table>
<p>For text or other non-numeric data, metrics such as the <a href="/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a> or <a href="/wiki/Levenshtein_distance" title="Levenshtein distance">Levenshtein distance</a> are often used.</p>
<p>A review of cluster analysis in health psychology research found that the most common distance measure in published studies in that research area is the Euclidean distance or the squared Euclidean distance.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2009)">citation needed</span></a></i>]</sup></p>
<h3><span class="mw-headline" id="Linkage_criteria">Linkage criteria</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=3" title="Edit section: Linkage criteria">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The linkage criterion determines the distance between sets of observations as a function of the pairwise distances between observations.</p>
<p>Some commonly used linkage criteria between two sets of observations <i>A</i> and <i>B</i> are:<sup id="cite_ref-5" class="reference"><a href="#cite_note-5"><span>[</span>5<span>]</span></a></sup><sup id="cite_ref-6" class="reference"><a href="#cite_note-6"><span>[</span>6<span>]</span></a></sup></p>
<table class="wikitable">
<tr>
<th>Names</th>
<th>Formula</th>
</tr>
<tr>
<td>Maximum or <a href="/wiki/Complete_linkage_clustering" title="Complete linkage clustering" class="mw-redirect">complete linkage clustering</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \max \, \{\, d(a,b) : a \in A,\, b \in B \,\}. " src="//upload.wikimedia.org/math/d/5/a/d5a349ae7f762ff9ceb08e7c1b75221c.png" /></td>
</tr>
<tr>
<td>Minimum or <a href="/wiki/Single-linkage_clustering" title="Single-linkage clustering">single-linkage clustering</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \min \, \{\, d(a,b) : a \in A,\, b \in B \,\}. " src="//upload.wikimedia.org/math/5/0/d/50dd90576e80aaa3104c2234959bf821.png" /></td>
</tr>
<tr>
<td>Mean or average linkage clustering, or <a href="/wiki/UPGMA" title="UPGMA">UPGMA</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" \frac{1}{|A| |B|} \sum_{a \in A }\sum_{ b \in B} d(a,b). " src="//upload.wikimedia.org/math/4/a/f/4af137bebc50f3168abaec48f45e6852.png" /></td>
</tr>
<tr>
<td>Centroid linkage clustering, or UPGMC</td>
<td><img class="mwe-math-fallback-image-inline tex" alt=" ||c_s - c_t || " src="//upload.wikimedia.org/math/6/c/9/6c971f4622a4d1f05aacdf7b584f5431.png" /> where <img class="mwe-math-fallback-image-inline tex" alt="c_s" src="//upload.wikimedia.org/math/d/1/c/d1ced7f9d157b8d7ee795880b78543dd.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="c_t" src="//upload.wikimedia.org/math/c/d/4/cd40b468a6890fe609be337d815300a3.png" /> are the centroids of clusters <i>s</i> and <i>t</i>, respectively.</td>
</tr>
<tr>
<td><a href="/wiki/Energy_distance" title="Energy distance">Minimum energy clustering</a></td>
<td><img class="mwe-math-fallback-image-inline tex" alt="  \frac {2}{nm}\sum_{i,j=1}^{n,m} \|a_i- b_j\|_2 - \frac {1}{n^2}\sum_{i,j=1}^{n} \|a_i-a_j\|_2 - \frac{1}{m^2}\sum_{i,j=1}^{m} \|b_i-b_j\|_2 " src="//upload.wikimedia.org/math/0/a/6/0a6e2bddb97160afb912393f23a08e2d.png" /></td>
</tr>
</table>
<p>where <i>d</i> is the chosen metric. Other linkage criteria include:</p>
<ul>
<li>The sum of all intra-cluster variance.</li>
<li>The decrease in variance for the cluster being merged (<a href="/wiki/Ward%27s_method" title="Ward's method">Ward's criterion</a>).<sup id="cite_ref-wards_method_7-0" class="reference"><a href="#cite_note-wards_method-7"><span>[</span>7<span>]</span></a></sup></li>
<li>The probability that candidate clusters spawn from the same distribution function (V-linkage).</li>
<li>The product of in-degree and out-degree on a k-nearest-neighbor graph (graph degree linkage).<sup id="cite_ref-8" class="reference"><a href="#cite_note-8"><span>[</span>8<span>]</span></a></sup></li>
<li>The increment of some cluster descriptor (i.e., a quantity defined for measuring the quality of a cluster) after merging two clusters.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9"><span>[</span>9<span>]</span></a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10"><span>[</span>10<span>]</span></a></sup><sup id="cite_ref-11" class="reference"><a href="#cite_note-11"><span>[</span>11<span>]</span></a></sup></li>
</ul>
<h2><span class="mw-headline" id="Discussion">Discussion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=4" title="Edit section: Discussion">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Hierarchical clustering has the distinct advantage that any valid measure of distance can be used. In fact, the observations themselves are not required: all that is used is a matrix of distances.</p>
<h2><span class="mw-headline" id="Example_for_Agglomerative_Clustering">Example for Agglomerative Clustering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=5" title="Edit section: Example for Agglomerative Clustering">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For example, suppose this data is to be clustered, and the <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is the <a href="/wiki/Metric_(mathematics)" title="Metric (mathematics)">distance metric</a>.</p>
<p>Cutting the tree at a given height will give a partitioning clustering at a selected precision. In this example, cutting after the second row of the dendrogram will yield clusters {a} {b c} {d e} {f}. Cutting after the third row will yield clusters {a} {b c} {d e f}, which is a coarser clustering, with a smaller number but larger clusters.</p>
<div class="thumb tnone">
<div class="thumbinner" style="width:252px;"><a href="/wiki/File:Clusters.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png" width="250" height="251" class="thumbimage" data-file-width="250" data-file-height="251" /></a>
<div class="thumbcaption">Raw data</div>
</div>
</div>
<p>The hierarchical clustering <a href="/wiki/Dendrogram" title="Dendrogram">dendrogram</a> would be as such:</p>
<div class="thumb tnone">
<div class="thumbinner" style="width:420px;"><a href="/wiki/File:Hierarchical_clustering_simple_diagram.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png" width="418" height="333" class="thumbimage" data-file-width="418" data-file-height="333" /></a>
<div class="thumbcaption">Traditional representation</div>
</div>
</div>
<p>This method builds the hierarchy from the individual elements by progressively merging clusters. In our example, we have six elements {a} {b} {c} {d} {e} and {f}. The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.</p>
<p>Optionally, one can also construct a <a href="/wiki/Distance_matrix" title="Distance matrix">distance matrix</a> at this stage, where the number in the <i>i</i>-th row <i>j</i>-th column is the distance between the <i>i</i>-th and <i>j</i>-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated. This is a common way to implement this type of clustering, and has the benefit of caching distances between clusters. A simple agglomerative clustering algorithm is described in the <a href="/wiki/Single-linkage_clustering" title="Single-linkage clustering">single-linkage clustering</a> page; it can easily be adapted to different types of linkage (see below).</p>
<p>Suppose we have merged the two closest elements <i>b</i> and <i>c</i>, we now have the following clusters {<i>a</i>}, {<i>b</i>, <i>c</i>}, {<i>d</i>}, {<i>e</i>} and {<i>f</i>}, and want to merge them further. To do that, we need to take the distance between {a} and {b c}, and therefore define the distance between two clusters. Usually the distance between two clusters <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{A}" src="//upload.wikimedia.org/math/8/4/c/84cc21a1ecbbe55e01e12e575a52cca2.png" /> and <img class="mwe-math-fallback-image-inline tex" alt="\mathcal{B}" src="//upload.wikimedia.org/math/8/d/7/8d7c27e339945f6c96cc234d1248d3fc.png" /> is one of the following:</p>
<ul>
<li>The maximum distance between elements of each cluster (also called <a href="/wiki/Complete-linkage_clustering" title="Complete-linkage clustering">complete-linkage clustering</a>):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" \max \{\, d(x,y) : x \in \mathcal{A},\, y \in \mathcal{B}\,\}. " src="//upload.wikimedia.org/math/f/3/6/f365699ecdd1c3e47e893a98f4b47bcd.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The minimum distance between elements of each cluster (also called <a href="/wiki/Single-linkage_clustering" title="Single-linkage clustering">single-linkage clustering</a>):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" \min \{\, d(x,y) : x \in \mathcal{A},\, y \in \mathcal{B} \,\}. " src="//upload.wikimedia.org/math/a/c/1/ac1a5f5c7af57d20aeec8140db3b8558.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The mean distance between elements of each cluster (also called average linkage clustering, used e.g. in <a href="/wiki/UPGMA" title="UPGMA">UPGMA</a>):</li>
</ul>
<dl>
<dd>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt=" {1 \over {|\mathcal{A}|\cdot|\mathcal{B}|}}\sum_{x \in \mathcal{A}}\sum_{ y \in \mathcal{B}} d(x,y). " src="//upload.wikimedia.org/math/7/8/f/78fa745ce59b8c5ea8b6b00f36cb97ae.png" /></dd>
</dl>
</dd>
</dl>
<ul>
<li>The sum of all intra-cluster variance.</li>
<li>The increase in variance for the cluster being merged (<a href="/wiki/Ward%27s_method" title="Ward's method">Ward's method</a><sup id="cite_ref-wards_method_7-1" class="reference"><a href="#cite_note-wards_method-7"><span>[</span>7<span>]</span></a></sup>)</li>
<li>The probability that candidate clusters spawn from the same distribution function (V-linkage).</li>
</ul>
<p>Each agglomeration occurs at a greater distance between clusters than the previous agglomeration, and one can decide to stop clustering either when the clusters are too far apart to be merged (distance criterion) or when there is a sufficiently small number of clusters (number criterion).</p>
<h2><span class="mw-headline" id="Software">Software</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=6" title="Edit section: Software">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Open_Source_Frameworks">Open Source Frameworks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=7" title="Edit section: Open Source Frameworks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a href="/wiki/R_(programming_language)" title="R (programming language)">R</a> has several functions for hierarchical clustering: see <a rel="nofollow" class="external text" href="http://cran.r-project.org/web/views/Cluster.html">CRAN Task View: Cluster Analysis &amp; Finite Mixture Models</a> for more information.</li>
<li><a rel="nofollow" class="external text" href="http://bonsai.hgc.jp/~mdehoon/software/cluster/">Cluster 3.0</a> provides a nice <a href="/wiki/Graphical_User_Interface" title="Graphical User Interface" class="mw-redirect">Graphical User Interface</a> to access to different clustering routines and is available for Windows, Mac OS X, Linux, Unix.</li>
<li><a href="/wiki/Environment_for_DeveLoping_KDD-Applications_Supported_by_Index-Structures" title="Environment for DeveLoping KDD-Applications Supported by Index-Structures" class="mw-redirect">ELKI</a> includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK<sup id="cite_ref-SLINK_2-1" class="reference"><a href="#cite_note-SLINK-2"><span>[</span>2<span>]</span></a></sup> algorithm, flexible cluster extraction from dendrograms and various other <a href="/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> algorithms.</li>
<li><a href="/wiki/GNU_Octave" title="GNU Octave">Octave</a>, the <a href="/wiki/GNU" title="GNU">GNU</a> analog to <a href="/wiki/MATLAB" title="MATLAB">MATLAB</a> implements hierarchical clustering in <a rel="nofollow" class="external text" href="http://octave.sourceforge.net/statistics/function/linkage.html">linkage function</a></li>
<li><a href="/wiki/Orange_(software)" title="Orange (software)">Orange</a>, a free data mining software suite, module <a rel="nofollow" class="external text" href="http://www.ailab.si/orange/doc/modules/orngClustering.htm">orngClustering</a> for scripting in <a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a>, or cluster analysis through visual programming.</li>
<li><a href="/wiki/Scikit-learn" title="Scikit-learn">scikit-learn</a> implements a hierarchical clustering.</li>
<li><a href="/wiki/Weka_(machine_learning)" title="Weka (machine learning)">Weka</a> includes hierarchical cluster analysis.</li>
<li><a href="/w/index.php?title=FastCluster&amp;action=edit&amp;redlink=1" class="new" title="FastCluster (page does not exist)">fastCluster</a> efficiently implements the seven most widely used clustering schemes.</li>
<li><a href="/wiki/SCaViS" title="SCaViS" class="mw-redirect">SCaViS</a> computing environment in Java that implements this algorithm.</li>
</ul>
<h3><span class="mw-headline" id="Standalone_implementations">Standalone implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=8" title="Edit section: Standalone implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a href="/wiki/CrimeStat" title="CrimeStat">CrimeStat</a> implements two hierarchical clustering routines, a nearest neighbor (Nnh) and a risk-adjusted(Rnnh).</li>
<li><a rel="nofollow" class="external text" href="http://code.google.com/p/figue/">figue</a> is a <a href="/wiki/JavaScript" title="JavaScript">JavaScript</a> package that implements some agglomerative clustering functions (single-linkage, complete-linkage, average-linkage) and functions to visualize clustering output (e.g. dendrograms).</li>
<li><a rel="nofollow" class="external text" href="http://code.google.com/p/scipy-cluster/">hcluster</a> is a <a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a> implementation, based on <a href="/wiki/NumPy" title="NumPy">NumPy</a>, which supports hierarchical clustering and plotting.</li>
<li><a rel="nofollow" class="external text" href="http://www.semanticsearchart.com/researchHAC.html">Hierarchical Agglomerative Clustering</a> implemented as C# visual studio project that includes real text files processing, building of document-term matrix with stop words filtering and stemming.</li>
<li><a rel="nofollow" class="external text" href="http://deim.urv.cat/~sergio.gomez/multidendrograms.php">MultiDendrograms</a> An <a href="/wiki/Open_source" title="Open source">open source</a> <a href="/wiki/Java_(programming_language)" title="Java (programming language)">Java</a> application for variable-group agglomerative hierarchical clustering, with <a href="/wiki/Graphical_user_interface" title="Graphical user interface">graphical user interface</a>.</li>
<li><a rel="nofollow" class="external text" href="http://www.mathworks.com/matlabcentral/fileexchange/38018-graph-agglomerative-clustering-gac-toolbox">Graph Agglomerative Clustering (GAC) toolbox</a> implemented several graph-based agglomerative clustering algorithms.</li>
<li><a rel="nofollow" class="external text" href="http://www.cs.umd.edu/hcil/hce/">Hierarchical Clustering Explorer</a> provides tools for interactive exploration of multidimensional data.</li>
</ul>
<h3><span class="mw-headline" id="Commercial">Commercial</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=9" title="Edit section: Commercial">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a href="/wiki/MathWorks" title="MathWorks">MATLAB</a> includes hierarchical cluster analysis.</li>
<li><a href="/wiki/SAS_System" title="SAS System" class="mw-redirect">SAS</a> includes hierarchical cluster analysis.</li>
<li><a href="/wiki/Mathematica" title="Mathematica">Mathematica</a> includes a Hierarchical Clustering Package.</li>
<li><a href="/wiki/NCSS_(statistical_software)" title="NCSS (statistical software)">NCSS (statistical software)</a> includes hierarchical cluster analysis.</li>
<li><a href="/wiki/SPSS" title="SPSS">SPSS</a> includes hierarchical cluster analysis.</li>
<li><a href="/wiki/Qlucore" title="Qlucore">Qlucore</a> Omics Explorer includes hierarchical cluster analysis.</li>
<li><a href="/wiki/Stata" title="Stata">Stata</a> includes hierarchical cluster analysis.</li>
</ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Statistical_distance" title="Statistical distance">Statistical distance</a></li>
<li><a href="/wiki/Brown_clustering" title="Brown clustering">Brown clustering</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></li>
<li><a href="/wiki/CURE_data_clustering_algorithm" title="CURE data clustering algorithm">CURE data clustering algorithm</a></li>
<li><a href="/wiki/Dendrogram" title="Dendrogram">Dendrogram</a></li>
<li><a href="/wiki/Determining_the_number_of_clusters_in_a_data_set" title="Determining the number of clusters in a data set">Determining the number of clusters in a data set</a></li>
<li><a href="/wiki/Hierarchical_clustering_of_networks" title="Hierarchical clustering of networks">Hierarchical clustering of networks</a></li>
<li><a href="/wiki/Nearest-neighbor_chain_algorithm" title="Nearest-neighbor chain algorithm">Nearest-neighbor chain algorithm</a></li>
<li><a href="/wiki/Numerical_taxonomy" title="Numerical taxonomy">Numerical taxonomy</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS algorithm</a></li>
<li><a href="/wiki/Nearest_neighbor_search" title="Nearest neighbor search">Nearest neighbor search</a></li>
<li><a href="/wiki/Locality-sensitive_hashing" title="Locality-sensitive hashing">Locality-sensitive hashing</a></li>
</ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=11" title="Edit section: Notes">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text">Rokach, Lior, and Oded Maimon. "Clustering methods." Data mining and knowledge discovery handbook. Springer US, 2005. 321-352.</span></li>
<li id="cite_note-SLINK-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-SLINK_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SLINK_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><span class="citation journal">R. Sibson (1973). <a rel="nofollow" class="external text" href="http://www.cs.gsu.edu/~wkim/index_files/papers/sibson.pdf">"SLINK: an optimally efficient algorithm for the single-link cluster method"</a> <span style="font-size:85%;">(PDF)</span>. <i>The Computer Journal</i> (British Computer Society) <b>16</b> (1): 30–34. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//dx.doi.org/10.1093%2Fcomjnl%2F16.1.30">10.1093/comjnl/16.1.30</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=SLINK%3A+an+optimally+efficient+algorithm+for+the+single-link+cluster+method&amp;rft.aulast=R.+Sibson&amp;rft.au=R.+Sibson&amp;rft.date=1973&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.gsu.edu%2F~wkim%2Findex_files%2Fpapers%2Fsibson.pdf&amp;rft_id=info%3Adoi%2F10.1093%2Fcomjnl%2F16.1.30&amp;rft.issue=1&amp;rft.jtitle=The+Computer+Journal&amp;rft.pages=30-34&amp;rft.pub=British+Computer+Society&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=16" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><span class="citation journal">D. Defays (1977). <a rel="nofollow" class="external text" href="http://comjnl.oxfordjournals.org/content/20/4/364.abstract">"An efficient algorithm for a complete link method"</a>. <i>The Computer Journal</i> (British Computer Society) <b>20</b> (4): 364–366. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//dx.doi.org/10.1093%2Fcomjnl%2F20.4.364">10.1093/comjnl/20.4.364</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=An+efficient+algorithm+for+a+complete+link+method&amp;rft.au=D.+Defays&amp;rft.aulast=D.+Defays&amp;rft.date=1977&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fcomjnl.oxfordjournals.org%2Fcontent%2F20%2F4%2F364.abstract&amp;rft_id=info%3Adoi%2F10.1093%2Fcomjnl%2F20.4.364&amp;rft.issue=4&amp;rft.jtitle=The+Computer+Journal&amp;rft.pages=364-366&amp;rft.pub=British+Computer+Society&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=20" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/statug_distance_sect016.htm">"The DISTANCE Procedure: Proximity Measures"</a>. <i>SAS/STAT 9.2 Users Guide</i>. <a href="/wiki/SAS_Institute" title="SAS Institute">SAS Institute</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-26</span></span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=The+DISTANCE+Procedure%3A+Proximity+Measures&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fsupport.sas.com%2Fdocumentation%2Fcdl%2Fen%2Fstatug%2F63033%2FHTML%2Fdefault%2Fstatug_distance_sect016.htm&amp;rft.jtitle=SAS%2FSTAT+9.2+Users+Guide&amp;rft.pub=SAS+Institute&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><span class="citation web"><a rel="nofollow" class="external text" href="http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/statug_cluster_sect012.htm">"The CLUSTER Procedure: Clustering Methods"</a>. <i>SAS/STAT 9.2 Users Guide</i>. <a href="/wiki/SAS_Institute" title="SAS Institute">SAS Institute</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-26</span></span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=The+CLUSTER+Procedure%3A+Clustering+Methods&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fsupport.sas.com%2Fdocumentation%2Fcdl%2Fen%2Fstatug%2F63033%2FHTML%2Fdefault%2Fstatug_cluster_sect012.htm&amp;rft.jtitle=SAS%2FSTAT+9.2+Users+Guide&amp;rft.pub=SAS+Institute&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Székely, G. J. and Rizzo, M. L. (2005) Hierarchical clustering via Joint Between-Within Distances: Extending Ward's Minimum Variance Method, Journal of Classification 22, 151-183.</span></li>
<li id="cite_note-wards_method-7"><span class="mw-cite-backlink">^ <a href="#cite_ref-wards_method_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-wards_method_7-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><span class="citation journal">Ward, Joe H. (1963). "Hierarchical Grouping to Optimize an Objective Function". <i>Journal of the American Statistical Association</i> <b>58</b> (301): 236–244. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//dx.doi.org/10.2307%2F2282967">10.2307/2282967</a>. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="//www.jstor.org/stable/2282967">2282967</a>. <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&#160;<a rel="nofollow" class="external text" href="//www.ams.org/mathscinet-getitem?mr=0148188">0148188</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=Hierarchical+Grouping+to+Optimize+an+Objective+Function&amp;rft.aufirst=Joe+H.&amp;rft.aulast=Ward&amp;rft.au=Ward%2C+Joe+H.&amp;rft.date=1963&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.2307%2F2282967&amp;rft.issue=301&amp;rft.jstor=2282967&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.mr=0148188&amp;rft.pages=236-244&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=58" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Zhang, et al. "Graph degree linkage: Agglomerative clustering on a directed graph." 12th European Conference on Computer Vision, Florence, Italy, October 7–13, 2012. <a rel="nofollow" class="external free" href="http://arxiv.org/abs/1208.5092">http://arxiv.org/abs/1208.5092</a></span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Zhang, et al. "Agglomerative clustering via maximum incremental path integral." Pattern Recognition (2013).</span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text">Zhao, and Tang. "Cyclizing clusters via zeta function of a graph."Advances in Neural Information Processing Systems. 2008.</span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">Ma, et al. "Segmentation of multivariate mixed data via lossy data coding and compression." IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(9) (2007): 1546-1562.</span></li>
</ol>
</div>
<h2><span class="mw-headline" id="References_and_further_reading">References and further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit&amp;section=12" title="Edit section: References and further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><span class="citation book">Kaufman, L.; Rousseeuw, P.J. (1990). <i>Finding Groups in Data: An Introduction to Cluster Analysis</i> (1 ed.). New York: John Wiley. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-471-87876-6" title="Special:BookSources/0-471-87876-6">0-471-87876-6</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.aufirst=L.&amp;rft.au=Kaufman%2C+L.&amp;rft.aulast=Kaufman&amp;rft.au=Rousseeuw%2C+P.J.&amp;rft.btitle=Finding+Groups+in+Data%3A+An+Introduction+to+Cluster+Analysis&amp;rft.date=1990&amp;rft.edition=1&amp;rft.genre=book&amp;rft.isbn=0-471-87876-6&amp;rft.place=New+York&amp;rft.pub=John+Wiley&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li><span class="citation book"><a href="/wiki/Trevor_Hastie" title="Trevor Hastie">Hastie, Trevor</a>; <a href="/wiki/Robert_Tibshirani" title="Robert Tibshirani">Tibshirani, Robert</a>; Friedman, Jerome (2009). "14.3.12 Hierarchical clustering". <a rel="nofollow" class="external text" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/"><i>The Elements of Statistical Learning</i></a> <span style="font-size:85%;">(PDF)</span> (2nd ed.). New York: Springer. pp.&#160;520–528. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-387-84857-6" title="Special:BookSources/0-387-84857-6">0-387-84857-6</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-10-20</span></span>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=14.3.12+Hierarchical+clustering&amp;rft.aufirst=Trevor&amp;rft.au=Friedman%2C+Jerome&amp;rft.au=Hastie%2C+Trevor&amp;rft.aulast=Hastie&amp;rft.au=Tibshirani%2C+Robert&amp;rft.btitle=The+Elements+of+Statistical+Learning&amp;rft.date=2009&amp;rft.edition=2nd&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fwww-stat.stanford.edu%2F~tibs%2FElemStatLearn%2F&amp;rft.isbn=0-387-84857-6&amp;rft.pages=520-528&amp;rft.place=New+York&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li><span class="citation book">Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). <a rel="nofollow" class="external text" href="http://apps.nrbook.com/empanel/index.html#pg=868">"Section 16.4. Hierarchical Clustering by Phylogenetic Trees"</a>. <i>Numerical Recipes: The Art of Scientific Computing</i> (3rd ed.). New York: Cambridge University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-521-88068-8" title="Special:BookSources/978-0-521-88068-8">978-0-521-88068-8</a>.</span><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHierarchical+clustering&amp;rft.atitle=Section+16.4.+Hierarchical+Clustering+by+Phylogenetic+Trees&amp;rft.aufirst=WH&amp;rft.au=Flannery%2C+BP&amp;rft.aulast=Press&amp;rft.au=Press%2C+WH&amp;rft.au=Teukolsky%2C+SA&amp;rft.au=Vetterling%2C+WT&amp;rft.btitle=Numerical+Recipes%3A+The+Art+of+Scientific+Computing&amp;rft.date=2007&amp;rft.edition=3rd&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fapps.nrbook.com%2Fempanel%2Findex.html%23pg%3D868&amp;rft.isbn=978-0-521-88068-8&amp;rft.place=New+York&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li><a rel="nofollow" class="external text" href="http://biostat.katerynakon.in.ua/en/cluster-analysis/hierarchical-cluster.html">Hierarchical Cluster Analysis</a></li>
<li><a rel="nofollow" class="external text" href="http://biostat.katerynakon.in.ua/en/free-software.html">Free statistical software. An overview of statistical software and methods used in published microbiological studies</a></li>
</ul>


<!-- 
NewPP limit report
Parsed by mw1134
CPU time usage: 0.344 seconds
Real time usage: 1.700 seconds
Preprocessor visited node count: 1086/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 34234/2097152 bytes
Template argument size: 1084/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 1/500
Lua time usage: 0.096/10.000 seconds
Lua memory usage: 2.96 MB/50 MB
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00% 1575.497      1 - -total
  5.11%   80.471      1 - Template:Machine_learning_bar
  4.90%   77.136      1 - Template:Reflist
  4.88%   76.816      1 - Template:Sidebar
  2.82%   44.491      3 - Template:Cite_journal
  2.72%   42.932      1 - Template:Citation_needed
  2.32%   36.542      1 - Template:Fix
  2.06%   32.404      2 - Template:Category_handler
  1.77%   27.841      3 - Template:Portal-inline
  1.12%   17.600      3 - Template:Cite_book
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:477573-0!*!0!!en!4!*!math=0 and timestamp 20150708224637 and revision id 670589093
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Hierarchical_clustering&amp;oldid=670589093">https://en.wikipedia.org/w/index.php?title=Hierarchical_clustering&amp;oldid=670589093</a>"					</div>
				<div id='catlinks' class='catlinks'><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Network_analysis" title="Category:Network analysis">Network analysis</a></li><li><a href="/wiki/Category:Data_clustering_algorithms" title="Category:Data clustering algorithms">Data clustering algorithms</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_April_2009" title="Category:Articles with unsourced statements from April 2009">Articles with unsourced statements from April 2009</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-createaccount"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Hierarchical+clustering&amp;type=signup" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Hierarchical+clustering" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li  id="ca-nstab-main" class="selected"><span><a href="/wiki/Hierarchical_clustering"  title="View the content page [c]" accesskey="c">Article</a></span></li>
															<li  id="ca-talk"><span><a href="/wiki/Talk:Hierarchical_clustering"  title="Discussion about the content page [t]" accesskey="t" rel="discussion">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label">
							<span>Variants</span><a href="#"></a>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="/wiki/Hierarchical_clustering" >Read</a></span></li>
															<li id="ca-edit"><span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=edit"  title="Edit this page [e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Hierarchical_clustering&amp;action=history"  title="Past versions of this page [h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input type="search" name="search" placeholder="Search" title="Search Wikipedia [f]" accesskey="f" id="searchInput" /><input type="hidden" value="Special:Search" name="title" /><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton" /><input type="submit" name="go" value="Go" title="Go to a page with this exact name if one exists" id="searchButton" class="searchButton" />							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikimedia Shop">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
			<h3 id='p-interaction-label'>Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Hierarchical_clustering" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Hierarchical_clustering" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Hierarchical_clustering&amp;oldid=670589093" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Hierarchical_clustering&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="//www.wikidata.org/wiki/Q1277447" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Hierarchical_clustering&amp;id=670589093" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
			<h3 id='p-coll-print_export-label'>Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Hierarchical+clustering">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Hierarchical+clustering&amp;oldid=670589093&amp;writer=rdf2latex">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Hierarchical_clustering&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
			<h3 id='p-lang-label'>Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-de"><a href="//de.wikipedia.org/wiki/Hierarchische_Clusteranalyse" title="Hierarchische Clusteranalyse – German" lang="de" hreflang="de">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="//es.wikipedia.org/wiki/Agrupamiento_jer%C3%A1rquico" title="Agrupamiento jerárquico – Spanish" lang="es" hreflang="es">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="//fr.wikipedia.org/wiki/Regroupement_hi%C3%A9rarchique" title="Regroupement hiérarchique – French" lang="fr" hreflang="fr">Français</a></li><li class="interlanguage-link interwiki-it"><a href="//it.wikipedia.org/wiki/Clustering_gerarchico" title="Clustering gerarchico – Italian" lang="it" hreflang="it">Italiano</a></li><li class="interlanguage-link interwiki-pl"><a href="//pl.wikipedia.org/wiki/Metoda_Czekanowskiego" title="Metoda Czekanowskiego – Polish" lang="pl" hreflang="pl">Polski</a></li><li class="interlanguage-link interwiki-ru"><a href="//ru.wikipedia.org/wiki/%D0%98%D0%B5%D1%80%D0%B0%D1%80%D1%85%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F" title="Иерархическая кластеризация – Russian" lang="ru" hreflang="ru">Русский</a></li><li class="interlanguage-link interwiki-uk"><a href="//uk.wikipedia.org/wiki/%D0%86%D1%94%D1%80%D0%B0%D1%80%D1%85%D1%96%D1%87%D0%BD%D0%B0_%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F" title="Ієрархічна кластеризація – Ukrainian" lang="uk" hreflang="uk">Українська</a></li><li class="uls-p-lang-dummy"><a href="#"></a></li>					</ul>
				<div class='after-portlet after-portlet-lang'><span class="wb-langlinks-edit wb-langlinks-link"><a href="//www.wikidata.org/wiki/Q1277447#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 8 July 2015, at 22:46.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="//wikimediafoundation.org/wiki/Privacy_policy" title="wikimedia:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Hierarchical_clustering&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="//wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>						</li>
											<li id="footer-poweredbyico">
							<a href="//www.mediawiki.org/"><img src="//en.wikipedia.org/static/1.26wmf13/resources/assets/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="//en.wikipedia.org/static/1.26wmf13/resources/assets/poweredby_mediawiki_132x47.png 1.5x, //en.wikipedia.org/static/1.26wmf13/resources/assets/poweredby_mediawiki_176x62.png 2x" width="88" height="31" /></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>if(window.jQuery)jQuery.ready();</script><script>if(window.mw){
mw.loader.state({"ext.globalCssJs.site":"ready","ext.globalCssJs.user":"ready","site":"loading","user":"ready","user.groups":"ready"});
}</script>
<link rel="stylesheet" href="//en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.gadget.DRN-wizard%2CReferenceTooltips%2CWatchlistGreenIndicators%2Ccharinsert%2Cfeatured-articles-links%2CrefToolbar%2Cswitcher%2Cteahouse%7Cext.wikimediaBadges&amp;only=styles&amp;skin=vector&amp;*" />
<script>if(window.mw){
mw.loader.load(["ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.cirrusSearch.loggingSchema","mmv.bootstrap.autostart","ext.imageMetrics.loader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.wikimediaEvents.statsd","ext.navigationTiming","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.WatchlistGreenIndicators","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.visualEditor.targetLoader","schema.UniversalLanguageSelector","ext.uls.eventlogger","ext.uls.interlanguage"],null,true);
}</script>
<script>if(window.mw){
document.write("\u003Cscript src=\"//en.wikipedia.org/w/load.php?debug=false\u0026amp;lang=en\u0026amp;modules=site\u0026amp;only=scripts\u0026amp;skin=vector\u0026amp;*\"\u003E\u003C/script\u003E");
}</script>
<script>if(window.mw){
mw.config.set({"wgBackendResponseTime":97,"wgHostname":"mw1070"});
}</script>
	</body>
</html>
